{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b003cb5",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7729f",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f53426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>PC29</th>\n",
       "      <th>PC30</th>\n",
       "      <th>PC31</th>\n",
       "      <th>PC32</th>\n",
       "      <th>PC33</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210370</td>\n",
       "      <td>-3.783125</td>\n",
       "      <td>-3.109984</td>\n",
       "      <td>-2.330564</td>\n",
       "      <td>0.194412</td>\n",
       "      <td>-0.106196</td>\n",
       "      <td>1.359164</td>\n",
       "      <td>-0.800872</td>\n",
       "      <td>-0.018861</td>\n",
       "      <td>-0.106735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.448110</td>\n",
       "      <td>-0.216595</td>\n",
       "      <td>0.238602</td>\n",
       "      <td>-0.064950</td>\n",
       "      <td>0.374568</td>\n",
       "      <td>0.152447</td>\n",
       "      <td>0.038673</td>\n",
       "      <td>-0.005687</td>\n",
       "      <td>0.284065</td>\n",
       "      <td>9.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.652259</td>\n",
       "      <td>-1.481696</td>\n",
       "      <td>-1.125819</td>\n",
       "      <td>2.124545</td>\n",
       "      <td>-0.359850</td>\n",
       "      <td>-0.727299</td>\n",
       "      <td>-0.421893</td>\n",
       "      <td>-0.488968</td>\n",
       "      <td>-0.470321</td>\n",
       "      <td>0.748475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183500</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>-0.063486</td>\n",
       "      <td>0.376771</td>\n",
       "      <td>-0.169494</td>\n",
       "      <td>-0.115262</td>\n",
       "      <td>-0.166771</td>\n",
       "      <td>-0.088883</td>\n",
       "      <td>-0.336320</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866014</td>\n",
       "      <td>-3.058621</td>\n",
       "      <td>-2.573576</td>\n",
       "      <td>-1.358566</td>\n",
       "      <td>0.107867</td>\n",
       "      <td>1.033776</td>\n",
       "      <td>0.713390</td>\n",
       "      <td>-0.566665</td>\n",
       "      <td>-0.056746</td>\n",
       "      <td>0.425253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078281</td>\n",
       "      <td>0.297077</td>\n",
       "      <td>0.336427</td>\n",
       "      <td>0.075464</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>-0.049016</td>\n",
       "      <td>0.092363</td>\n",
       "      <td>-0.015393</td>\n",
       "      <td>-0.100124</td>\n",
       "      <td>8.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.691269</td>\n",
       "      <td>-0.402958</td>\n",
       "      <td>0.439577</td>\n",
       "      <td>-0.756925</td>\n",
       "      <td>-0.220367</td>\n",
       "      <td>0.326274</td>\n",
       "      <td>-0.581471</td>\n",
       "      <td>-1.383755</td>\n",
       "      <td>0.853735</td>\n",
       "      <td>-0.420171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036835</td>\n",
       "      <td>0.182699</td>\n",
       "      <td>-0.114823</td>\n",
       "      <td>0.318219</td>\n",
       "      <td>-0.046057</td>\n",
       "      <td>-0.087436</td>\n",
       "      <td>-0.472599</td>\n",
       "      <td>0.064285</td>\n",
       "      <td>-0.018419</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.843302</td>\n",
       "      <td>-0.335543</td>\n",
       "      <td>-0.126733</td>\n",
       "      <td>-1.664559</td>\n",
       "      <td>0.824550</td>\n",
       "      <td>-0.191094</td>\n",
       "      <td>-0.474833</td>\n",
       "      <td>-0.662604</td>\n",
       "      <td>-0.009798</td>\n",
       "      <td>-0.112947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208560</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>0.172369</td>\n",
       "      <td>-0.155116</td>\n",
       "      <td>0.340742</td>\n",
       "      <td>0.103013</td>\n",
       "      <td>-0.019645</td>\n",
       "      <td>-0.131348</td>\n",
       "      <td>-0.153660</td>\n",
       "      <td>9.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>-1.228603</td>\n",
       "      <td>-0.642169</td>\n",
       "      <td>-0.511690</td>\n",
       "      <td>2.357222</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>2.118894</td>\n",
       "      <td>1.133076</td>\n",
       "      <td>-1.983986</td>\n",
       "      <td>1.294645</td>\n",
       "      <td>-0.747971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222962</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.090781</td>\n",
       "      <td>0.129622</td>\n",
       "      <td>0.087587</td>\n",
       "      <td>0.148446</td>\n",
       "      <td>0.129731</td>\n",
       "      <td>-0.326210</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>8.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>5.776524</td>\n",
       "      <td>-0.193315</td>\n",
       "      <td>1.423830</td>\n",
       "      <td>-0.006551</td>\n",
       "      <td>-0.832585</td>\n",
       "      <td>0.248954</td>\n",
       "      <td>-1.536620</td>\n",
       "      <td>-0.245970</td>\n",
       "      <td>-0.007010</td>\n",
       "      <td>-1.118094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317216</td>\n",
       "      <td>0.178732</td>\n",
       "      <td>-0.037934</td>\n",
       "      <td>-0.227529</td>\n",
       "      <td>0.203615</td>\n",
       "      <td>-0.227141</td>\n",
       "      <td>-0.661057</td>\n",
       "      <td>0.350377</td>\n",
       "      <td>0.033586</td>\n",
       "      <td>9.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>6.397876</td>\n",
       "      <td>5.206743</td>\n",
       "      <td>-1.936534</td>\n",
       "      <td>-0.469356</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>-0.688453</td>\n",
       "      <td>-0.433237</td>\n",
       "      <td>0.020591</td>\n",
       "      <td>0.173213</td>\n",
       "      <td>0.567808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>-0.763843</td>\n",
       "      <td>-0.001832</td>\n",
       "      <td>0.110674</td>\n",
       "      <td>0.044971</td>\n",
       "      <td>0.151868</td>\n",
       "      <td>0.395583</td>\n",
       "      <td>0.054981</td>\n",
       "      <td>-0.038778</td>\n",
       "      <td>10.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2.305427</td>\n",
       "      <td>7.295340</td>\n",
       "      <td>-0.521168</td>\n",
       "      <td>0.069176</td>\n",
       "      <td>0.365588</td>\n",
       "      <td>-1.432064</td>\n",
       "      <td>0.930092</td>\n",
       "      <td>-1.407111</td>\n",
       "      <td>-0.348041</td>\n",
       "      <td>-0.465773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134273</td>\n",
       "      <td>-0.065842</td>\n",
       "      <td>-0.189253</td>\n",
       "      <td>-0.713009</td>\n",
       "      <td>0.120962</td>\n",
       "      <td>-0.210204</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.295065</td>\n",
       "      <td>-0.132540</td>\n",
       "      <td>10.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>-2.229622</td>\n",
       "      <td>-2.217173</td>\n",
       "      <td>-1.528796</td>\n",
       "      <td>-1.568946</td>\n",
       "      <td>-0.339839</td>\n",
       "      <td>0.570581</td>\n",
       "      <td>2.219586</td>\n",
       "      <td>-1.040177</td>\n",
       "      <td>0.627816</td>\n",
       "      <td>1.367560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316692</td>\n",
       "      <td>-0.019878</td>\n",
       "      <td>0.046533</td>\n",
       "      <td>0.215087</td>\n",
       "      <td>-0.055606</td>\n",
       "      <td>-0.830402</td>\n",
       "      <td>0.224313</td>\n",
       "      <td>-0.146860</td>\n",
       "      <td>-0.372376</td>\n",
       "      <td>8.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0    0.210370 -3.783125 -3.109984 -2.330564  0.194412 -0.106196  1.359164   \n",
       "1   -4.652259 -1.481696 -1.125819  2.124545 -0.359850 -0.727299 -0.421893   \n",
       "2    0.866014 -3.058621 -2.573576 -1.358566  0.107867  1.033776  0.713390   \n",
       "3   -3.691269 -0.402958  0.439577 -0.756925 -0.220367  0.326274 -0.581471   \n",
       "4   -2.843302 -0.335543 -0.126733 -1.664559  0.824550 -0.191094 -0.474833   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "543 -1.228603 -0.642169 -0.511690  2.357222  0.040193  2.118894  1.133076   \n",
       "544  5.776524 -0.193315  1.423830 -0.006551 -0.832585  0.248954 -1.536620   \n",
       "545  6.397876  5.206743 -1.936534 -0.469356 -0.005460 -0.688453 -0.433237   \n",
       "546  2.305427  7.295340 -0.521168  0.069176  0.365588 -1.432064  0.930092   \n",
       "547 -2.229622 -2.217173 -1.528796 -1.568946 -0.339839  0.570581  2.219586   \n",
       "\n",
       "          PC8       PC9      PC10  ...      PC25      PC26      PC27  \\\n",
       "0   -0.800872 -0.018861 -0.106735  ... -0.448110 -0.216595  0.238602   \n",
       "1   -0.488968 -0.470321  0.748475  ... -0.183500  0.387231 -0.063486   \n",
       "2   -0.566665 -0.056746  0.425253  ...  0.078281  0.297077  0.336427   \n",
       "3   -1.383755  0.853735 -0.420171  ... -0.036835  0.182699 -0.114823   \n",
       "4   -0.662604 -0.009798 -0.112947  ... -0.208560 -0.025368  0.172369   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "543 -1.983986  1.294645 -0.747971  ... -0.222962  0.718315  0.090781   \n",
       "544 -0.245970 -0.007010 -1.118094  ... -0.317216  0.178732 -0.037934   \n",
       "545  0.020591  0.173213  0.567808  ...  0.127434 -0.763843 -0.001832   \n",
       "546 -1.407111 -0.348041 -0.465773  ...  0.134273 -0.065842 -0.189253   \n",
       "547 -1.040177  0.627816  1.367560  ...  0.316692 -0.019878  0.046533   \n",
       "\n",
       "         PC28      PC29      PC30      PC31      PC32      PC33  target  \n",
       "0   -0.064950  0.374568  0.152447  0.038673 -0.005687  0.284065    9.11  \n",
       "1    0.376771 -0.169494 -0.115262 -0.166771 -0.088883 -0.336320    9.04  \n",
       "2    0.075464  0.003606 -0.049016  0.092363 -0.015393 -0.100124    8.94  \n",
       "3    0.318219 -0.046057 -0.087436 -0.472599  0.064285 -0.018419    9.15  \n",
       "4   -0.155116  0.340742  0.103013 -0.019645 -0.131348 -0.153660    9.62  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "543  0.129622  0.087587  0.148446  0.129731 -0.326210  0.141956    8.91  \n",
       "544 -0.227529  0.203615 -0.227141 -0.661057  0.350377  0.033586    9.79  \n",
       "545  0.110674  0.044971  0.151868  0.395583  0.054981 -0.038778   10.61  \n",
       "546 -0.713009  0.120962 -0.210204 -0.054649 -0.295065 -0.132540   10.93  \n",
       "547  0.215087 -0.055606 -0.830402  0.224313 -0.146860 -0.372376    8.84  \n",
       "\n",
       "[548 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "kins = ['JAK1', 'JAK2', 'JAK3', 'TYK2']\n",
    "\n",
    "train_file_names = {}\n",
    "test_file_names = {}\n",
    "\n",
    "train_dfs = {}\n",
    "test_dfs = {}\n",
    "\n",
    "for kin in kins:\n",
    "    train_file_names[kin] = \"data/train_\" + kin + \".csv\"\n",
    "    test_file_names[kin] = \"data/test_\" + kin + \".csv\"\n",
    "    \n",
    "    train_dfs[kin] = pd.read_csv(train_file_names[kin]).drop(columns=['Unnamed: 0'])\n",
    "    test_dfs[kin] = pd.read_csv(test_file_names[kin]).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "display(train_dfs['JAK1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4e234",
   "metadata": {},
   "source": [
    "### Build MLPs for each kinase and optimize hyperparameters\n",
    "Here we find the optimal hyperparameters for each of the networks. The ones we are tuning for are number of hidden layers, number of neurons per layer, and learning rate. I've included the dropout rate to avoid overfitting, as we are opening up the hidden layer exploration range all the way up to 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d02d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 00m 02s]\n",
      "val_mean_squared_error: 1.2417775392532349\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.4018065631389618\n",
      "Total elapsed time: 00h 00m 31s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |64                |neurons\n",
      "3                 |5                 |layers\n",
      "0.01              |0.01              |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.3391 - mean_squared_error: 19.3391 - val_loss: 5.6891 - val_mean_squared_error: 5.6891\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 5.2938 - mean_squared_error: 5.2938 - val_loss: 5.0098 - val_mean_squared_error: 5.0098\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 4.8658 - mean_squared_error: 4.8658 - val_loss: 4.4043 - val_mean_squared_error: 4.4043\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 4.0021 - mean_squared_error: 4.0021 - val_loss: 2.3352 - val_mean_squared_error: 2.3352\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 3.7134 - mean_squared_error: 3.7134 - val_loss: 3.1061 - val_mean_squared_error: 3.1061\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 3.4314 - mean_squared_error: 3.4314 - val_loss: 6.2165 - val_mean_squared_error: 6.2165\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 4.0994 - mean_squared_error: 4.0994 - val_loss: 2.1457 - val_mean_squared_error: 2.1457\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.8456 - mean_squared_error: 2.8456 - val_loss: 2.4139 - val_mean_squared_error: 2.4139\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.5281 - mean_squared_error: 2.5281 - val_loss: 1.9324 - val_mean_squared_error: 1.9324\n",
      "Epoch 10/50\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 4.9516 - mean_squared_error: 4.9516"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    # Input layer\n",
    "    model.add(keras.layers.Dense(33))\n",
    "\n",
    "    # Tune the number of neurons in each layer, and the number of layers\n",
    "    hp_neurons = hp.Choice('neurons', values=[64, 128, 256, 512])\n",
    "    hp_layers = hp.Choice('layers', values = range(3, 17))\n",
    "    while hp_layers > 0:\n",
    "        model.add(keras.layers.Dense(hp_neurons, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "        hp_layers -= 1\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mean_squared_error'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_best_hyperparameters(kin, train_data):\n",
    "    x_train = train_data.drop(columns=['target']).to_numpy(dtype=float)\n",
    "    y_train = train_data['target'].to_numpy(dtype=float)\n",
    "    print(x_train.shape)\n",
    "    tuner = kt.BayesianOptimization(model_builder,\n",
    "                                    objective=kt.Objective(\"val_mean_squared_error\", \"min\"),\n",
    "                                    overwrite=True,\n",
    "                                    max_trials=10\n",
    "                                   )\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mean_squared_error', patience=10000)\n",
    "\n",
    "    tuner.search(x_train, y_train, epochs=50, validation_split=0.2)\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    hp_dict = {\n",
    "        'neurons': best_hps.get('neurons'),\n",
    "        'layers': best_hps.get('layers'),\n",
    "        'learning': best_hps.get('learning_rate')\n",
    "    }\n",
    "    return hp_dict\n",
    "\n",
    "optimal_hps = {}\n",
    "for kin in kins:\n",
    "    optimal_hps[kin] = get_best_hyperparameters(kin, train_dfs[kin])\n",
    "\n",
    "for x in optimal_hps:\n",
    "    print(x)\n",
    "    print(optimal_hps[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977dc2d",
   "metadata": {},
   "source": [
    "### Evaluate model performance on test data\n",
    "We will be using mean squared error as the loss function, as it's important to reduce large outlier errors in the pKi. Outliers could result in completely missed strong bindings, and sometimes assumption of a strong binding when there is little to no strength in actuality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_neurons, num_layers, learning_rate):\n",
    "    layer_seq = []\n",
    "    layer_seq.append(keras.layers.Dense(33))\n",
    "    while num_layers > 0:\n",
    "        layer_seq.append(keras.layers.Dense(num_neurons, activation='relu', kernel_initializer='he_uniform'))\n",
    "        layer_seq.append(keras.layers.Dropout(0.2))\n",
    "        num_layers -= 1\n",
    "    \n",
    "    layer_seq.append(keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "    model = keras.Sequential(layer_seq)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(kin, train_data, test_data, optimal_hps):\n",
    "    x_train = train_data.drop(columns=['target']).to_numpy(dtype=float)\n",
    "    y_train = train_data['target'].to_numpy(dtype=float)\n",
    "    \n",
    "    x_test = test_data.drop(columns=['target']).to_numpy(dtype=float)\n",
    "    y_test = test_data['target'].to_numpy(dtype=float)\n",
    "    model = build_model(optimal_hps[kin]['neurons'], optimal_hps[kin]['layers'], optimal_hps[kin]['learning'])\n",
    "    monitor = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=1000,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    \n",
    "    evaluation = model.fit(x_train,\n",
    "            y_train,\n",
    "            epochs=10000,\n",
    "            verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[monitor])\n",
    "    print(\"Evaluate on test data for \" + kin)\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"Test MSE:\", results)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
